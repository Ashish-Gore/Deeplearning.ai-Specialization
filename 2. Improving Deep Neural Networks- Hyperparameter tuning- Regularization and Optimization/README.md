# Course 2. Improving Deep Neural Networks- Hyperparameter tuning- Regularization and Optimization
Certificate: [Verify Here](https://coursera.org/share/4375d62473f9557f51d47beb827e2545)

![picture alt](https://github.com/Ashish-Gore/Deeplearning.ai-Specialization/blob/main/2.%20Improving%20Deep%20Neural%20Networks-%20Hyperparameter%20tuning-%20Regularization%20and%20Optimization/Course-2%20%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization.jpg)

## Objectives:
  - Recall that different types of initializations lead to different results
  - Recognize the importance of initialization in complex neural networks.
  - Recognize the difference between train/dev/test sets
  - Diagnose the bias and variance issues in your model
  - Learn when and how to use regularization methods such as dropout or L2 regularization.
  - Understand experimental issues in deep learning such as Vanishing or Exploding gradients and learn how to deal with them
  - Use gradient checking to verify the correctness of your backpropagation implementation
  - Remember different optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam
  - Use random minibatches to accelerate the convergence and improve the optimization
  - Know the benefits of learning rate decay and apply it to your optimization
  - Master the process of hyperparameter tuning
  
## Week 1:
 - Initialization.ipynb
 - Regularization.ipynb
 - Gradient Checking.ipynb
 
 ## Week 2:
  - Optimization methods.ipynb
 
 ## Week 3:
  - Tensorflow Tutorial.ipynb
